{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT AND REDUCTION OF THE ORIGINAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "input_file = \"itineraries.csv\"\n",
    "chunksize = 10000000  \n",
    "\n",
    "def get_month(date_str):\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%d\").month\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "outputs = {\n",
    "    \"all\": {\n",
    "        \"first_file\": True,\n",
    "        \"csv_name\": \"file_all_LAX_data.csv\",\n",
    "        \"table\": \"all_data\",\n",
    "        \"data\": []\n",
    "    },\n",
    "    \"august\": {\n",
    "        \"first_file\": True,\n",
    "        \"csv_name\": \"file_TEST_LAX_august.csv\",\n",
    "        \"table\": \"august_data\",\n",
    "        \"data\": []\n",
    "    },\n",
    "    \"apr_jul\": {\n",
    "        \"first_file\": True,\n",
    "        \"csv_name\": \"file_TRAINING__LAX_april_july.csv\",\n",
    "        \"table\": \"april_july_data\",\n",
    "        \"data\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "conn = sqlite3.connect(\"LAX_data.db\")\n",
    "\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "    filtered_chunk = chunk[chunk.iloc[:, 3] == \"LAX\"].copy()\n",
    "    if filtered_chunk.empty:\n",
    "        continue\n",
    "\n",
    "    filtered_chunk.loc[:, \"Month\"] = filtered_chunk.iloc[:, 2].apply(get_month)\n",
    "\n",
    "    for key, config in outputs.items():\n",
    "        if key == \"all\":\n",
    "            data_to_save = filtered_chunk.copy()\n",
    "        elif key == \"august\":\n",
    "            data_to_save = filtered_chunk[filtered_chunk[\"Month\"] == 8].copy()\n",
    "        elif key == \"apr_jul\":\n",
    "            data_to_save = filtered_chunk[filtered_chunk[\"Month\"].between(4, 7)].copy()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not data_to_save.empty:\n",
    "            data_to_save = data_to_save.drop(columns=[\"Month\"])\n",
    "            config[\"data\"].append(data_to_save.copy())  \n",
    "\n",
    "            data_to_save.to_csv(config[\"csv_name\"], mode='a', index=False, header=config[\"first_file\"])\n",
    "            data_to_save.to_sql(config[\"table\"], conn, if_exists='append', index=False)\n",
    "\n",
    "            config[\"first_file\"] = False\n",
    "\n",
    "conn.close()\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3400829, 27)\n",
      "(1742260, 27)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Concatenate accumulated data from memory\n",
    "df_train = pd.concat(outputs[\"apr_jul\"][\"data\"], ignore_index=True)\n",
    "df_test = pd.concat(outputs[\"august\"][\"data\"], ignore_index=True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values, Drop features, De-duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_flight_data(df):\n",
    "    \"\"\"\n",
    "    Cleans the flight data by:\n",
    "    1. Dropping rows with any missing (NaN) values\n",
    "    2. Removing rows where specific segment columns are structurally invalid\n",
    "    3. Removing rows where any part of specific segment columns contains 'None'\n",
    "    4. Removing rows with non-numeric or negative values\n",
    "    5. Removing rows with invalid dates\n",
    "    6. Dropping unnecessary columns\n",
    "    7. Removing fully duplicated rows\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Drop rows with any missing (NaN) values\n",
    "    df_clean = df.dropna()\n",
    "\n",
    "    # 2. Remove rows where specific segment columns are structurally invalid\n",
    "    def is_structurally_invalid(val):\n",
    "        try:\n",
    "            val = str(val).strip()\n",
    "            if val == \"||\":\n",
    "                return True\n",
    "            parts = val.split(\"||\")\n",
    "            if len(parts) == 2:\n",
    "                return parts[0].strip() == \"\" or parts[1].strip() == \"\"\n",
    "            return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    multi_segment_cols = [\n",
    "        'segmentsArrivalTimeEpochSeconds',\n",
    "        'segmentsArrivalTimeRaw',\n",
    "        'segmentsArrivalAirportCode',\n",
    "        'segmentsDepartureAirportCode',\n",
    "        'segmentsAirlineName',\n",
    "        'segmentsAirlineCode',\n",
    "        'segmentsEquipmentDescription',\n",
    "        'segmentsDurationInSeconds',\n",
    "        'segmentsDistance',\n",
    "        'segmentsCabinCode'\n",
    "    ]\n",
    "\n",
    "    for col in multi_segment_cols:\n",
    "        df_clean = df_clean[~df_clean[col].apply(is_structurally_invalid)]\n",
    "\n",
    "    # 3. Remove rows where any part of specific segment columns contains 'None'\n",
    "    def has_any_none_value(val):\n",
    "        if pd.isna(val):\n",
    "            return True\n",
    "        parts = str(val).split(\"||\")\n",
    "        return any(p.strip() == \"None\" for p in parts)\n",
    "\n",
    "    none_sensitive_cols = [\n",
    "        'segmentsDistance',\n",
    "        'segmentsDurationInSeconds',\n",
    "        'segmentsDepartureTimeEpochSeconds',\n",
    "        'segmentsArrivalTimeEpochSeconds'\n",
    "    ]\n",
    "\n",
    "    for col in none_sensitive_cols:\n",
    "        df_clean = df_clean[~df_clean[col].apply(has_any_none_value)]\n",
    "    \n",
    "    # 4. Remove rows with non-numeric or negative values\n",
    "    def is_not_valid_number(val):\n",
    "        try:\n",
    "            float(val)\n",
    "            return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    def has_invalid_split_values(val):\n",
    "        try:\n",
    "            parts = str(val).split(\"||\")\n",
    "            for p in parts:\n",
    "                if p.strip() == \"\":\n",
    "                    continue\n",
    "                if is_not_valid_number(p.strip()) or float(p.strip()) < 0:\n",
    "                    return True\n",
    "            return False\n",
    "        except:\n",
    "            return True\n",
    "\n",
    "    numeric_cols = [\n",
    "        'totalTravelDistance',\n",
    "        'seatsRemaining',\n",
    "        'elapsedDays',\n",
    "        'baseFare'\n",
    "    ]\n",
    "\n",
    "    initial_len = len(df_clean)\n",
    "\n",
    "    invalid_mask = df_clean[numeric_cols].applymap(is_not_valid_number)\n",
    "    neg_mask = df_clean[numeric_cols] < 0\n",
    "    combined_invalid = invalid_mask | neg_mask\n",
    "    to_remove = combined_invalid.any(axis=1)\n",
    "    df_clean = df_clean[~to_remove]\n",
    "    removed_numeric = to_remove.sum()\n",
    "\n",
    "    segment_cols = [\n",
    "        'segmentsDepartureTimeEpochSeconds',\n",
    "        'segmentsArrivalTimeEpochSeconds',\n",
    "        'segmentsDurationInSeconds',\n",
    "        'segmentsDistance'\n",
    "    ]\n",
    "\n",
    "    removed_segments = {}\n",
    "    for col in segment_cols:\n",
    "        mask = df_clean[col].apply(has_invalid_split_values)\n",
    "        count = mask.sum()\n",
    "        df_clean = df_clean[~mask]\n",
    "        removed_segments[col] = count\n",
    "    \n",
    "    # 5. Remove rows with invalid dates (flightDate and segments*Raw)\n",
    "    # 5-1. Remove rows with invalid dates in 'flightDate' and 'searchDate'\n",
    "    #      - Parse using pd.to_datetime(errors='coerce')\n",
    "    #      - If parsing fails (invalid date), it becomes NaT â†’ remove such rows\n",
    "\n",
    "    for date_col in ['flightDate', 'searchDate']:\n",
    "        df_clean['__parsed_date'] = pd.to_datetime(df_clean[date_col], errors='coerce')\n",
    "        df_clean = df_clean[df_clean['__parsed_date'].notna()].drop(columns='__parsed_date')\n",
    "\n",
    "    # 5-2. Check if any of the 'segments*Raw' fields contain invalid date parts\n",
    "    #       - Split values by '||'\n",
    "    #       - Extract date portion (before 'T')\n",
    "    #       - Remove row if any extracted date is invalid\n",
    "    def has_invalid_raw_date(val):\n",
    "        try:\n",
    "            parts = str(val).split(\"||\")\n",
    "            for p in parts:\n",
    "                if not p.strip():\n",
    "                    continue\n",
    "                date_str = p.strip().split(\"T\")[0]\n",
    "                if pd.to_datetime(date_str, errors=\"coerce\") is pd.NaT:\n",
    "                    return True\n",
    "            return False\n",
    "        except:\n",
    "            return True  # If parsing fails, treat as invalid\n",
    "\n",
    "    raw_date_cols = ['segmentsDepartureTimeRaw', 'segmentsArrivalTimeRaw']\n",
    "    for col in raw_date_cols:\n",
    "        mask = df_clean[col].apply(has_invalid_raw_date)\n",
    "        df_clean = df_clean[~mask]\n",
    "\n",
    "    # 6. Drop unnecessary columns\n",
    "    cols_to_drop = [\n",
    "        'totalFare',\n",
    "        'legId'\n",
    "        #'isRefundable', -> will remove later\n",
    "        #'segmentsAirlineName' -> will remove later\n",
    "        #'segmentsDepartureTimeRaw', -> will remove later\n",
    "        #'segmentsArrivalTimeRaw', -> will remove later\n",
    "    ]\n",
    "    df_clean = df_clean.drop(columns = cols_to_drop, errors = 'ignore')\n",
    "\n",
    "    # 7. Drop fully duplicated rows (identical in all columns)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mats\\AppData\\Local\\Temp\\ipykernel_16084\\2775977997.py:91: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  invalid_mask = df_clean[numeric_cols].applymap(is_not_valid_number)\n",
      "C:\\Users\\Mats\\AppData\\Local\\Temp\\ipykernel_16084\\2775977997.py:91: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  invalid_mask = df_clean[numeric_cols].applymap(is_not_valid_number)\n"
     ]
    }
   ],
   "source": [
    "df_train = clean_flight_data(df_train)\n",
    "df_test = clean_flight_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2689889, 25)\n",
      "(1383413, 25)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Destination Airport One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# One-Hot encoding\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "\n",
    "# train\n",
    "encoded_train = pd.DataFrame(\n",
    "    encoder.fit_transform(df_train[['destinationAirport']]).toarray(), \n",
    "    columns=encoder.get_feature_names_out(['destinationAirport']),\n",
    "    index=df_train.index\n",
    ")\n",
    "\n",
    "# test\n",
    "encoded_test = pd.DataFrame(\n",
    "    encoder.transform(df_test[['destinationAirport']]).toarray(), \n",
    "    columns=encoder.get_feature_names_out(['destinationAirport']),\n",
    "    index=df_test.index\n",
    ")\n",
    "\n",
    "df_train = df_train.join(encoded_train)\n",
    "df_test = df_test.join(encoded_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Travel Duration to Minutes\n",
    "Total travel time from departure til arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: isodate in c:\\users\\mats\\anaconda3\\lib\\site-packages (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install isodate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isodate\n",
    "\n",
    "# Convert duration to minutes using isodate\n",
    "def convert_duration_to_min(df):\n",
    "    df['travelDuration_minutes'] = df['travelDuration'].apply(lambda x: isodate.parse_duration(x).total_seconds() / 60)\n",
    "    return df\n",
    "\n",
    "df_train = convert_duration_to_min(df_train)\n",
    "df_test = convert_duration_to_min(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Convert searchDate and flightDate to ï¼®umerical Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_to_numerical(df, reference_min_dates):\n",
    "    for feature in reference_min_dates:\n",
    "        df[feature] = pd.to_datetime(df[feature])\n",
    "        df[f'{feature}_as_int'] = (df[feature] - reference_min_dates[feature]).dt.days + 1\n",
    "    return df\n",
    "\n",
    "# Compute training reference dates\n",
    "ref_dates = {\n",
    "    'searchDate': pd.to_datetime(df_train['searchDate']).min(),\n",
    "    'flightDate': pd.to_datetime(df_train['flightDate']).min()\n",
    "}\n",
    "\n",
    "df_train = convert_date_to_numerical(df_train, ref_dates)\n",
    "df_test = convert_date_to_numerical(df_test, ref_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isHoliday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: holidays in c:\\users\\mats\\anaconda3\\lib\\site-packages (0.71)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\mats\\anaconda3\\lib\\site-packages (from holidays) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mats\\anaconda3\\lib\\site-packages (from python-dateutil->holidays) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mats\\AppData\\Local\\Temp\\ipykernel_16084\\1523949206.py:8: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  df['is_holiday'] = df['flightDate'].isin(us_holidays)\n",
      "C:\\Users\\Mats\\AppData\\Local\\Temp\\ipykernel_16084\\1523949206.py:8: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  df['is_holiday'] = df['flightDate'].isin(us_holidays)\n"
     ]
    }
   ],
   "source": [
    "import holidays\n",
    "\n",
    "# Get US holidays for 2022\n",
    "us_holidays = holidays.US(years=[2022])\n",
    "\n",
    "def is_holiday(df):\n",
    "    # check holiday\n",
    "    df['is_holiday'] = df['flightDate'].isin(us_holidays)\n",
    "\n",
    "    # Convert boolean column to binary\n",
    "    df['is_holiday'] = df['is_holiday'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = is_holiday(df_train)\n",
    "df_test = is_holiday(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isAroundHoliday\n",
    "+-3 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_around_holiday(df):\n",
    "    # Add a column to check if flightDate is a holiday or close to one (Â±3 days)\n",
    "    df['isAroundHoliday'] = df['flightDate'].apply(\n",
    "        lambda x: any((x - pd.Timedelta(days=delta)) in us_holidays for delta in range(-3, 4))\n",
    "    )\n",
    "\n",
    "    # Convert boolean column to binary\n",
    "    df['isAroundHoliday'] = df['isAroundHoliday'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = is_around_holiday(df_train)\n",
    "df_test = is_around_holiday(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dayOfWeek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_week(df):\n",
    "    #create dayOfWeek\n",
    "    df['dayOfWeekNum'] = df['flightDate'].dt.weekday\n",
    "\n",
    "    # Convert from int32 to int64\n",
    "    df['dayOfWeekNum'] = df['dayOfWeekNum'].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = day_of_week(df_train)\n",
    "df_test = day_of_week(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dayOfWeek search day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_week2(df):\n",
    "    #create dayOfWeek\n",
    "    df['SearchdayOfWeekNum'] = df['searchDate'].dt.weekday\n",
    "\n",
    "    # Convert from int32 to int64\n",
    "    df['SearchdayOfWeekNum'] = df['SearchdayOfWeekNum'].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = day_of_week2(df_train)\n",
    "df_test = day_of_week2(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Month of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_of_year(df):\n",
    "    # Create Month of the year\n",
    "    df['monthNum'] = df['flightDate'].dt.month\n",
    "\n",
    "    # Convert from int32 to int64\n",
    "    df['monthNum'] = df['monthNum'].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = month_of_year(df_train)\n",
    "df_test = month_of_year(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_month(df):\n",
    "    # Create Month of the year\n",
    "    df['dayNum'] = df['flightDate'].dt.day\n",
    "\n",
    "    # Convert from int32 to int64\n",
    "    df['dayNum'] = df['dayNum'].astype('int64')\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = day_of_month(df_train)\n",
    "df_test = day_of_month(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Days until the flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_until_flight(df):\n",
    "\n",
    "    df['daysUntilFlight'] = (df['flightDate'] - df['searchDate']).dt.days\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = days_until_flight(df_train)\n",
    "df_test = days_until_flight(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of legs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_legs(df):\n",
    "    #Create number of legs\n",
    "    df['num_legs'] = df['segmentsArrivalTimeEpochSeconds'].apply(\n",
    "        lambda x : len(str(x).split('||')) if pd.notnull(x) else 0\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = num_of_legs(df_train)\n",
    "df_test = num_of_legs(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Departure time and Arrival time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get first departure hour\n",
    "def get_first_hour(time_str):\n",
    "    first_time = time_str.split(\"||\")[0]\n",
    "    return pd.to_datetime(first_time).hour\n",
    "\n",
    "# Function to get last arrival hour\n",
    "def get_last_hour(time_str):\n",
    "    last_time = time_str.split(\"||\")[-1]\n",
    "    return pd.to_datetime(last_time).hour\n",
    "\n",
    "def departure_arrival_time(df):\n",
    "    # Apply the functions\n",
    "    df[\"departureHour\"] = df[\"segmentsDepartureTimeRaw\"].apply(get_first_hour)\n",
    "    df[\"arrivalHour\"] = df[\"segmentsArrivalTimeRaw\"].apply(get_last_hour)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = departure_arrival_time(df_train)\n",
    "df_test = departure_arrival_time(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total transfer time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create transfer time\n",
    "\n",
    "def calculate_transfer_time(row):\n",
    "    dep_times = row['segmentsDepartureTimeRaw'].split('||')\n",
    "    arr_times = row['segmentsArrivalTimeRaw'].split('||')\n",
    "    \n",
    "    # Convert to datetime\n",
    "    dep_times = [pd.to_datetime(t) for t in dep_times]\n",
    "    arr_times = [pd.to_datetime(t) for t in arr_times]\n",
    "    \n",
    "    # If single leg, no transfer time\n",
    "    if len(dep_times) <= 1:\n",
    "        return 0\n",
    "\n",
    "    # Calculate transfer times (i.e., next dep - prev arr)\n",
    "    transfer_times = [\n",
    "        (dep_times[i] - arr_times[i - 1]).total_seconds()\n",
    "        for i in range(1, len(dep_times))\n",
    "    ]\n",
    "    \n",
    "    # Sum all transfer times\n",
    "    total_transfer_seconds = sum(transfer_times)\n",
    "    return total_transfer_seconds / 60  # in minutes\n",
    "\n",
    "def total_transfer_time(df):\n",
    "    df['totalTransferTime'] = df.apply(calculate_transfer_time, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = total_transfer_time(df_train)\n",
    "df_test = total_transfer_time(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TotalFlightTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_flight_time(df):\n",
    "    #Create totalFlightTime\n",
    "    df['totalFlightTime'] = df['travelDuration_minutes'] - df['totalTransferTime']\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = total_flight_time(df_train)\n",
    "df_test = total_flight_time(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create total distance\n",
    "\n",
    "def safe_sum(segment):\n",
    "    if not isinstance(segment, str):\n",
    "        return 0\n",
    "    try:\n",
    "        return sum(int(s) for s in segment.split('||') if s.isdigit())\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def total_distance(df):\n",
    "    df['total_distance'] = df['segmentsDistance'].apply(safe_sum)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = total_distance(df_train)\n",
    "df_test = total_distance(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airlines with the longest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Airlines with the longest distance\n",
    "\n",
    "# Assuming the 'isNonStop' column is a boolean that tells whether the flight is non-stop.\n",
    "# We also assume the 'segmentsAirlineName' contains the airline names for each leg of the trip.\n",
    "\n",
    "def get_airline_longest_distance_or_direct(row):\n",
    "    # Check if the flight is non-stop\n",
    "    if row['isNonStop']:\n",
    "        # If it's a direct flight, return the first airline name from segmentsAirlineName\n",
    "        airlines = row['segmentsAirlineName'].split('||')  # Split airlines by '||'\n",
    "        return airlines[0] if airlines else None  # Return the first airline if available\n",
    "    else:\n",
    "        # If it's not a direct flight, process the segments to find the airline with the longest distance\n",
    "        if isinstance(row['segmentsDistance'], str) and isinstance(row['segmentsAirlineName'], str):\n",
    "            distances = row['segmentsDistance'].split('||')\n",
    "            airlines = row['segmentsAirlineName'].split('||')\n",
    "            \n",
    "            # Convert distances to integers, handling non-numeric values\n",
    "            valid_distances = []\n",
    "            for distance in distances:\n",
    "                try:\n",
    "                    valid_distances.append(int(distance))\n",
    "                except ValueError:\n",
    "                    valid_distances.append(None)  # Append None for invalid values\n",
    "            \n",
    "            # Check if we have valid distances\n",
    "            if valid_distances:\n",
    "                # Find the index of the segment with the longest distance (ignore None values)\n",
    "                longest_distance_index = max(\n",
    "                    range(len(valid_distances)), key=lambda i: (valid_distances[i] if valid_distances[i] is not None else -1)\n",
    "                )\n",
    "                # Get the airline corresponding to the longest distance\n",
    "                airline_with_longest_distance = airlines[longest_distance_index]\n",
    "                return airline_with_longest_distance\n",
    "            else:\n",
    "                return None  # If all distances are invalid, return None\n",
    "        else:\n",
    "            return None  # If the data is missing or not valid, return None\n",
    "\n",
    "# Apply the function to create the new 'longestDistanceAirline' column\n",
    "#df['longestDistanceAirline'] = df.apply(get_airline_longest_distance_or_direct, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airline frequency-encoding weighted by distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def airline_count_weighted_encoding(df, airline_col='segmentsAirlineName', distance_col='segmentsDistance', sep='||'):\n",
    "    \"\"\"\n",
    "    Encodes each row into airline features by:\n",
    "    1. Counting how many times each airline appears,\n",
    "    2. Multiplying by total distance per airline,\n",
    "    3. Normalizing the row so values sum to 1.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with airline columns added.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Function to parse a single row into airlines and distances\n",
    "    def parse_row(airline_str, distance_str):\n",
    "        # Split airlines and distances by the delimiter (e.g., '||')\n",
    "        airlines = re.split(r'\\|+', airline_str) if pd.notnull(airline_str) else []\n",
    "        raw_distances = re.split(r'\\|+', distance_str) if pd.notnull(distance_str) else []\n",
    "\n",
    "        # Convert distance strings to floats and handle potential errors\n",
    "        distances = []\n",
    "        for d in raw_distances:\n",
    "            try:\n",
    "                distances.append(float(d.strip())) # Convert distance to float\n",
    "            except:\n",
    "                distances.append(0.0) # Handle any invalid distance values by setting them to 0.0\n",
    "        \n",
    "        # Ensure both lists are the same length (by trimming the longer list)\n",
    "        n = min(len(airlines), len(distances))\n",
    "        return airlines[:n], distances[:n]\n",
    "    \n",
    "    # Initialize a list to store the encoded feature rows\n",
    "    feature_rows = []\n",
    "\n",
    "    # Set to store all unique airlines encountered\n",
    "    all_airlines = set()\n",
    "    \n",
    "    # Iterate over each row of the dataframe (may takes a long time)\n",
    "    for _, row in df.iterrows():\n",
    "        # Parse the airlines and distances for the current row\n",
    "        airlines, distances = parse_row(row[airline_col], row[distance_col])\n",
    "\n",
    "        # Dictionaries to store the count and total distance for each airline\n",
    "        airline_counts = defaultdict(int) # Default to 0 for counts\n",
    "        airline_distances = defaultdict(float) # Default to 0.0 for distances\n",
    "\n",
    "        # Accumulate counts and total distance per airline\n",
    "        for a, d in zip(airlines, distances):\n",
    "            airline_counts[a] += 1 # Increment the count for this airline\n",
    "            airline_distances[a] += d # Add the distance to the airline's total\n",
    "            all_airlines.add(a) # Add the airline to the set of all airlines\n",
    "\n",
    "        # Compute the weighted values: airline count * total distance\n",
    "        weighted_values = {\n",
    "            a: airline_counts[a] * airline_distances[a] for a in airline_counts\n",
    "        }\n",
    "\n",
    "        # Calculate the total weighted value across all airlines in the row\n",
    "        total = sum(weighted_values.values()) or 1  # avoid division by zero\n",
    "\n",
    "        # Normalize the weighted values so the sum of the row equals 1\n",
    "        normalized = {a: weighted_values.get(a, 0.0) / total for a in all_airlines}\n",
    "\n",
    "        # Append the normalized values to the feature rows list\n",
    "        feature_rows.append(normalized)\n",
    "    \n",
    "    # Convert the list of feature rows into a DataFrame\n",
    "    feature_df = pd.DataFrame(feature_rows).fillna(0)\n",
    "    return pd.concat([df.reset_index(drop=True), feature_df], axis=1)\n",
    "\n",
    "\n",
    "df_train = airline_count_weighted_encoding(df_train)\n",
    "df_test = airline_count_weighted_encoding(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relBoing, relAirbus, relOther"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_weighted_aircraft_fractions(equipment_str, distance_str, total_distance):\n",
    "    if not isinstance(equipment_str, str) or not isinstance(distance_str, str):\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    aircraft_list = equipment_str.split('||')\n",
    "    distance_list = distance_str.split('||')\n",
    "\n",
    "    try:\n",
    "        distances = [float(d) for d in distance_list]\n",
    "    except ValueError:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    boeing_dist = sum(dist for a, dist in zip(aircraft_list, distances) if 'boeing' in a.lower())\n",
    "    airbus_dist = sum(dist for a, dist in zip(aircraft_list, distances) if 'airbus' in a.lower())\n",
    "    others_dist = total_distance - boeing_dist - airbus_dist\n",
    "\n",
    "    rel_boeing = boeing_dist / total_distance\n",
    "    rel_airbus = airbus_dist / total_distance\n",
    "    rel_others = others_dist / total_distance\n",
    "\n",
    "    return rel_boeing, rel_airbus, rel_others\n",
    "\n",
    "def aircraft_relative_counts(df):\n",
    "    # Apply the function \n",
    "    df['relBoeing'], df['relAirbus'], df['relOthers'] = zip(*df.apply(\n",
    "        lambda row: compute_weighted_aircraft_fractions(\n",
    "            row['segmentsEquipmentDescription'], \n",
    "            row['segmentsDistance'],\n",
    "            row['total_distance']\n",
    "        ), \n",
    "        axis=1\n",
    "    ))\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = aircraft_relative_counts(df_train)\n",
    "df_test = aircraft_relative_counts(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### isCoach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to verify if at least one part of the trip is \"coach\"\n",
    "def is_coach(cabin_code):\n",
    "    parts = cabin_code.lower().split('||')\n",
    "    return any(part.strip() == 'coach' for part in parts)\n",
    "\n",
    "def is_coach_check(df):\n",
    "    # Creating column isCoach:\n",
    "    df['isCoach'] = df['segmentsCabinCode'].apply(is_coach)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = is_coach_check(df_train)\n",
    "df_test = is_coach_check(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert boolean to binary encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_boolean_to_numerical(df):\n",
    "    df['isBasicEconomy'] = df['isBasicEconomy'].astype(int)\n",
    "    df['isRefundable'] = df['isRefundable'].astype(int)\n",
    "    df['isNonStop'] = df['isNonStop'].astype(int)\n",
    "    df['isCoach'] = df['isCoach'].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df_train = convert_boolean_to_numerical(df_train)\n",
    "df_test = convert_boolean_to_numerical(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unusable features (Keep it in the end before normalization!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    'startingAirport',\n",
    "    'fareBasisCode',\n",
    "    'totalTravelDistance',\n",
    "    'segmentsDepartureTimeEpochSeconds',\n",
    "    'segmentsDepartureTimeRaw',\n",
    "    'segmentsArrivalTimeEpochSeconds',\n",
    "    'segmentsArrivalTimeRaw',\n",
    "    'segmentsArrivalAirportCode',\n",
    "    'segmentsDepartureAirportCode',\n",
    "    'segmentsAirlineName',\n",
    "    'segmentsAirlineCode',\n",
    "    'segmentsEquipmentDescription',\n",
    "    'segmentsDurationInSeconds',\n",
    "    'segmentsDistance',\n",
    "    'segmentsCabinCode',\n",
    "    'isRefundable', \n",
    "    'total_distance_check',\n",
    "    'longestDistanceAirline',\n",
    "    'destinationAirport',\n",
    "    'travelDuration',\n",
    "    'searchDate', \n",
    "    'flightDate',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    df_cleaned = df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = drop_columns(df_train)\n",
    "df_test = drop_columns(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAVE FINAL TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('preprocessed_train3.csv', index=False)\n",
    "df_test.to_csv('preprocessed_test3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2689889\n"
     ]
    }
   ],
   "source": [
    "total_rows = len(df_train)\n",
    "print(total_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
