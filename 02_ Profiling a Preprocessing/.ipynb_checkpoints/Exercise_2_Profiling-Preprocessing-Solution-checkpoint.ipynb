{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you are using Google Colab to mount your Google Drive in your Colab instance. Adjust the path to the files in your Google Drive as needed if it differs.\n",
    "\n",
    "If you do not use Google Colab, running the cell will simply do nothing, so do not worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    %cd 'drive/My Drive/Colab Notebooks/06_Regression'\n",
    "except ImportError as e:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling, Preprocessing, and some PCA\n",
    "\n",
    "## 1. **Profiling**\n",
    "   - Involves **understanding the structure, distribution, and quality** of the dataset.\n",
    "   - Key tasks include **identifying missing values, checking for outliers, and understanding data types**.\n",
    "   - Profiling helps in gaining **insights about feature relevance, potential noise, or issues with data** that may require cleaning.\n",
    "\n",
    "## 2. **Preprocessing**\n",
    "   - Refers to **cleaning and transforming data** before using it.\n",
    "   - Common steps include **handling missing values, encoding categorical features, scaling numerical data, and normalizing distributions**.\n",
    "   - Preprocessing ensures that the data is in a **suitable form for analysis and model training**.\n",
    "\n",
    "## 3. **Principal Component Analysis (PCA)**\n",
    "   - A dimensionality reduction technique used to **simplify datasets** by transforming features into uncorrelated principal components.\n",
    "   - PCA reduces complexity **while preserving as much variance as possible**, aiding in visualization and improving model performance.\n",
    "   - It is useful when dealing with high-dimensional datasets, helping to **mitigate issues like overfitting or multicollinearity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will use a dataset about **WINE** which you can find in **data/wine.csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/wine1.png.jpg\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Read in the data and perform basic exploratory analysis\n",
    "\n",
    "df = pd.read_csv('data/wine.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition: Selecting and filtering data with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Repetition 1:** Cole wants to get wasted tonight!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/wine2.png.jpg\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_alcohol_row = df.loc[df[\"Alcohol\"].idxmax()]\n",
    "print(f\"{highest_alcohol_row['Label']} contains the most alcohol with {highest_alcohol_row['Alcohol']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Repetition 2:** Don't be like Cindy.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/wine3.png.jpg\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cindy_row = df[(df[\"Malic Acid\"] > 3.3) & (df[\"Ash Content\"] <= 2.6) & (df[\"Color Intensity\"] < 2.5)].iloc[0]\n",
    "print(f\"{cindy_row['Label']} would satisfy Cindy with its malic acid of {cindy_row['Malic Acid']}, a color intensity of {cindy_row['Color Intensity']}, and its ash content of {cindy_row['Ash Content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling: Learn to understand your data better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into data mining techniques, it is crucial to **profile** the dataset to gain insights into its structure and distribution. One of the fundamental steps in data profiling is computing **descriptive statistics for numerical features**.\n",
    "\n",
    "The `df.describe()` function provides:\n",
    "- **Count**: Number of non-missing values.\n",
    "- **Mean**: The average value.\n",
    "- **Standard Deviation (std)**: A measure of variability.\n",
    "- **Min & Max**: The range of values.\n",
    "- **Quartiles (25%, 50%, 75%)**: Key percentiles indicating data spread.\n",
    "\n",
    "This step helps detect **outliers, missing values, scaling issues, or skewness**, all of which influence downstream modeling and preprocessing decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While numerical features provide statistical insights, **non-numeric (categorical or textual) features** are equally important in data profiling. These features often require specific preprocessing steps such as encoding, handling missing values, and identifying unique categories.\n",
    "\n",
    "Key aspects to analyze for non-numeric features:\n",
    "- **Unique values**: Understanding distinct categories.\n",
    "- **Frequency distribution**: Checking for imbalances.\n",
    "- **Missing values**: Identifying potential data gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Categorical Features in Data Mining\n",
    "\n",
    "Categorical features require different preprocessing techniques depending on their properties. In this dataset, we categorize the columns as follows:\n",
    "\n",
    "- **`Label`**: Each value is unique, meaning it might act as an identifier rather than a useful categorical feature. If needed for analysis, **NLP techniques** (such as text vectorization) could be applied.\n",
    "- **`Grape Type`**: Since it has only **two unique values**, it will be **one-hot encoded**, converting it into binary indicator variables.\n",
    "- **`Class`**: This will be our **target variable** for classification and is label-encoded\n",
    "\n",
    "Splitting and encoding these features properly is crucial for ensuring the dataset is suitable for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping 'Label' since it is unique and not useful for standard encoding\n",
    "df = df.drop(columns=['Label'])\n",
    "\n",
    "# One-hot encode 'Grape Type'\n",
    "df = pd.get_dummies(df, columns=['Grape Type'], dtype=int)\n",
    "\n",
    "# Display results\n",
    "print(\"Processed Feature DataFrame:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis in Data Mining Pipelines\n",
    "\n",
    "Understanding relationships between features is a crucial step in data preprocessing. Correlation analysis helps in:\n",
    "\n",
    "- Identifying **strong predictors** of the target variable.\n",
    "- Detecting **multicollinearity**, where two or more features are highly correlated.\n",
    "- Finding **redundant or irrelevant features** that contribute little to predictive performance.\n",
    "- Avoiding **false predictors**, which correlate with the target due to chance but lack causal meaning.\n",
    "\n",
    "We will use **three different approaches** to analyze feature correlation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Scatter Plots with Target Coding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Iterate over numerical features and plot them against the target\n",
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.stripplot(x=df['Class'], y=df[col], hue=df['Class'], jitter=True, alpha=0.7, palette=\"viridis\")\n",
    "    plt.title(f\"Strip Plot of {col} by Class\", fontsize=14)\n",
    "    plt.xlabel(\"Class\", fontsize=12)\n",
    "    plt.ylabel(col, fontsize=12)\n",
    "    plt.xticks(rotation=45)  # Rotate labels if needed\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Boxplots by Target Class**\n",
    "- Examines how feature distributions vary across target categories.\n",
    "- Detects **features with strong class separation** and **potential outliers**.\n",
    "- If all class distributions overlap, the feature may not be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes(include=['number']).columns:\n",
    "    df.boxplot(col, by='Class',figsize=(7,4),fontsize=12)\n",
    "    plt.title(\"{}\\n\".format(col),fontsize=16)\n",
    "    plt.xlabel(\"Wine Class\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Scatter Plots with Target Color Coding**\n",
    "- Visualizes the relationship between numerical features.\n",
    "- The target variable (`Class`) is color-coded to see how well the separation occurs.\n",
    "- Helps identify **linear, non-linear, or no correlation** patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define unique class labels and generate distinct colors\n",
    "class_labels = df['Class'].unique()  # Get unique class labels\n",
    "num_classes = len(class_labels)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_classes))  # Generate distinct colors evenly spaced in colormap\n",
    "color_map = dict(zip(class_labels, colors))  # Map labels to colors\n",
    "\n",
    "# Assign correct colors to each point\n",
    "color = df[\"Class\"].map(color_map)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "scatter = plt.scatter(\n",
    "    df['Color Intensity'], df['Flavonoids'], \n",
    "    c=color.tolist(),  # Use mapped colors directly\n",
    "    edgecolors='k', alpha=0.75, s=150\n",
    ")\n",
    "\n",
    "# Create legend with correct colors\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10) \n",
    "           for color in colors]\n",
    "plt.legend(handles, class_labels, title=\"Class Labels\", fontsize=12, title_fontsize=13)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.title(\"Scatter plot of two features showing the \\ncorrelation and class separation\", fontsize=15)\n",
    "plt.xlabel(\"Color Intensity\", fontsize=15)\n",
    "plt.ylabel(\"Flavonoids\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Correlation Matrix (Heatmap)**\n",
    "- Computes Pearson correlations between numerical features.\n",
    "- Identifies **highly correlated features (multicollinearity)**, which can lead to overfitting.\n",
    "- Checks whether any features **correlate with the target**, indicating predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation matrix\n",
    "corr_matrix = df.select_dtypes(include=['number']).corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Matrix Without Target\", fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "df.select_dtypes(include=['number']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode 'Class' without affecting numerical features\n",
    "df_encoded = df.copy()  # Create a copy to preserve the original DataFrame\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['Class'], dtype=int)\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_encoded.select_dtypes(include=['number']).corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Matrix With Target\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **How to Decide Whether to Keep Features After Correlation Analysis?**\n",
    "\n",
    "Correlation analysis helps in identifying relationships between features and the target variable. However, just because a feature is **highly correlated** does not always mean it is **useful**. Some features might be **redundant, misleading, or unnecessary** for prediction. Here‚Äôs how to decide which features to keep:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations for Feature Selection**\n",
    "#### **1Ô∏è‚É£ Remove Highly Correlated Features (Multicollinearity)**\n",
    "- If two features are strongly correlated (e.g., **correlation > 0.8**), one of them is likely redundant.\n",
    "- Keeping both may lead to **overfitting** and **unstable model performance**.\n",
    "- **Solution:** Drop one of the two features or apply dimensionality reduction techniques like **PCA**.\n",
    "\n",
    "#### **2Ô∏è‚É£ Remove Features with No Correlation to the Target**\n",
    "- If a feature has **low or no correlation** with the target, it likely does not contribute to prediction.\n",
    "- Example: A feature that is **randomly distributed** across classes.\n",
    "- **Solution:** Use statistical tests (e.g., ANOVA, chi-square) to confirm irrelevance and remove such features.\n",
    "\n",
    "#### **3Ô∏è‚É£ Detect and Remove \"False Predictors\"**\n",
    "- A feature might **appear predictive** but is actually **deterministic** based on domain knowledge.\n",
    "- **Example from Our Scenario:**  \n",
    "  - The feature **‚ÄúGrape Type = Barbera‚Äù** perfectly predicts **Barbera wine**.\n",
    "  - However, this is **a definition, not a useful predictor**‚ÄîBarbera wine is always made from Barbera grapes.\n",
    "  - If we can **always** rely on having grape type information, **this feature alone is sufficient** to classify Barbera wines.\n",
    "  - **BUT:** If grape type information is sometimes missing, we must **remove this false predictor** and rely on other distinguishing features.\n",
    "\n",
    "---\n",
    "\n",
    "### **üõ† Final Decision in Our Scenario: Removing Grape Type Entirely**\n",
    "- Instead of just removing \"Grape Type = Barbera,\" we **remove all entries** from the **Grape Type** column.  \n",
    "- Therefore, we assume that information about the grape type is not accessible.\n",
    "\n",
    "This ensures that our model remains focused on distinguishing **Wine Classes** without relying on misleading or redundant predictors.\n",
    "\n",
    "---\n",
    "\n",
    "### **‚úÖ Best Practices for Feature Selection**\n",
    "‚úî **Use correlation analysis** to detect redundant or irrelevant features.  \n",
    "‚úî **Apply domain knowledge** to identify false predictors.  \n",
    "‚úî **Test feature importance** using statistical tests or feature selection methods.  \n",
    "‚úî **Consider feature availability**‚Äîkeep features that will always be available in real-world applications.  \n",
    "‚úî **Reframe the problem when necessary**‚Äîif a feature makes classification trivial, reconsider whether certain data points should be included at all.  \n",
    "\n",
    "By following these steps, we ensure that our model remains **interpretable, efficient, and reliable** in predicting wine classification. üç∑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/wine.csv')\n",
    "\n",
    "# Drop the \"Grape Type\" columns (since it's now unnecessary)\n",
    "df = df.drop(columns=['Grape Type'], errors='ignore')\n",
    "\n",
    "# Display the first few rows of the cleaned dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should officially perform a profiling again. However, to save some time, we can also use external tools such as ydata-profiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ydata-profiling\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report')\n",
    "profile.to_file(\"profile_report.html\")\n",
    "\n",
    "# open file in explorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Handling Missing Values in Data Mining**\n",
    "\n",
    "Missing values are common in datasets and can significantly impact model performance. Before filling in missing values, we need to **understand their distribution and potential causes**.\n",
    "\n",
    "### **Why Do Missing Values Occur?**\n",
    "- **Data entry errors** (e.g., manual input mistakes).\n",
    "- **Sensor failures** in automated data collection.\n",
    "- **Intentional omissions** (e.g., survey respondents skipping questions).\n",
    "- **Merging datasets** where some records lack specific information.\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/wine4.png.jpg\" style=\"width: 75%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the matching row\n",
    "print(df[df['Label'] == 'Brachetto Campania 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Identifying Missing Values**\n",
    "Before handling missing values, we need to **detect** them and understand their extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_summary = df.isnull().sum()\n",
    "\n",
    "# Display only features with missing values\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "print(\"Missing Values per Column:\\n\", missing_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Strategies to Handle Missing Values**\n",
    "Once we identify missing values, we can choose an appropriate strategy based on the nature of the data.\n",
    "\n",
    "### **Removing Rows or Columns**\n",
    "‚úî Best when a feature has **too many missing values** (e.g., >50%).  \n",
    "‚úî Also useful if **only a few rows** contain missing data and their removal **won‚Äôt significantly impact** the dataset.\n",
    "\n",
    "### **Imputation (Filling Missing Values)**\n",
    "Instead of removing data, we can **fill in missing values** using different techniques:\n",
    "\n",
    "| Strategy | When to Use | Example |\n",
    "|----------|------------|---------|\n",
    "| Mean / Median Imputation | Numerical features with **few missing values** | Filling missing temperatures with average temperature |\n",
    "| Mode Imputation | Categorical features with **low variability** | Replacing missing `City` with the most common city |\n",
    "| Forward / Backward Fill (Time Series) | When missing values follow a sequential pattern | Filling gaps in stock prices using previous values |\n",
    "| Interpolation | When trends exist in numeric data | Predicting missing values between known points |\n",
    "| KNN Imputation | When similar instances exist in the dataset | Using nearest neighbors to estimate missing values |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary copy of df\n",
    "df_tmp = df.copy()\n",
    "\n",
    "# Perform mean imputation for all numerical columns\n",
    "df_tmp.fillna(df_tmp.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# Print the matching row\n",
    "print(df_tmp[df_tmp['Label'] == 'Brachetto Campania 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train-Test Splitting and Fitting a KNN Predictor**\n",
    "\n",
    "### **Why Do We Need a Train-Test Split?**\n",
    "Before training a model, we need to **split** our dataset into:\n",
    "- **Training Set** ‚Üí Used to train the model.\n",
    "- **Test Set** ‚Üí Used to evaluate model performance on unseen data.\n",
    "\n",
    "This prevents **overfitting**, where a model performs well on known data but fails on new data.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"imgs/wine5.png.jpg\" style=\"width: 75%;\">\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['Label'] == 'Immobile Totti 2017'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we need to split our dataset into Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Class'], errors='ignore')\n",
    "y = df['Class']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 1: Splitting Data into Training and Test Sets**\n",
    "We will:\n",
    "- **Separate features (`X`) and the target (`y`)**  \n",
    "- **Perform an 80-20 train-test split**  \n",
    "- **Use stratification** to ensure class balance in both splits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print('Num of total entries:', len(X))\n",
    "print('Num of train entries:', len(X_train))\n",
    "print('Num of test entries:', len(X_test))\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "missing_summary = X_train.isnull().sum()\n",
    "\n",
    "# Display only features with missing values\n",
    "missing_summary = missing_summary[missing_summary > 0]\n",
    "print(\"Missing Values per Column:\\n\", missing_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform mean imputation for all numerical columns IN THE TRAINING DATA\n",
    "X_train.fillna(X_train.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# Print the matching row\n",
    "print(X_train[X_train['Label'] == 'Brachetto Campania 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** When missing values are filled by only looking at the training data, these filler (obviously) differ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Training a k-Nearest Neighbors (k-NN) Classifier**\n",
    "We will train a **k-NN model** using **ONLY** the training set. We set **k=3** and we will also derive key characteristics regarding its performance on the test data which is **NOT** used in the training procedure:\n",
    "\n",
    "- Measure **accuracy** to see how well the model classifies new instances.  \n",
    "- Compute a **confusion matrix** to analyze misclassifications.  \n",
    "- Use a **classification report** for precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=['Label'], errors='ignore')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train.fillna(X_train.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "def predict_knn(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Initialize the K-NN classifier (with k=5, you can adjust the number of neighbors)\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    \n",
    "    # Train the model\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Measure performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')  # 'weighted' handles multi-class labels\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Output results\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nClassification Report:\\n\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_knn(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of Scaling Numeric Data in Machine Learning\n",
    "\n",
    "## Why Scale Your Data?\n",
    "Scaling numeric data is an essential preprocessing step in machine learning. Many algorithms perform better when features have a consistent range and distribution. This is especially crucial for models that rely on distance metrics (e.g., k-NN, SVMs) or gradient-based optimization (e.g., neural networks, logistic regression).\n",
    "\n",
    "## Types of Scaling\n",
    "\n",
    "There are several common scaling techniques, each suitable for different scenarios:\n",
    "\n",
    "### 1Ô∏è‚É£ Standardization (StandardScaler)\n",
    "- Transforms data to have **zero mean** and **unit variance**:\n",
    "  $$\n",
    "  X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
    "  $$\n",
    "- Useful when data follows a **normal distribution**.\n",
    "\n",
    "### 2Ô∏è‚É£ Min-Max Scaling (MinMaxScaler)\n",
    "- Scales data to a fixed range, usually **[0,1]**:\n",
    "  $$\n",
    "  X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "  $$\n",
    "- Preserves relative relationships but **sensitive to outliers**.\n",
    "\n",
    "### 3Ô∏è‚É£ Robust Scaling (RobustScaler)\n",
    "- Uses the **median** and **interquartile range** (IQR), making it robust to **outliers**:\n",
    "  $$\n",
    "  X_{\\text{scaled}} = \\frac{X - \\text{median}(X)}{\\text{IQR}(X)}\n",
    "  $$\n",
    "- Suitable for datasets with extreme values.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è **Avoid Data Leakage: Never Fit a Scaler on the Full Dataset!**\n",
    "One of the most critical mistakes in machine learning is **data leakage**‚Äîwhen information from the test set influences the training process. **This must never happen!**\n",
    "\n",
    "### ‚ùå Bad Practice:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(X)  # Wrong! Includes test data\n",
    "```\n",
    "\n",
    "### ‚úÖ Good Practice:\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) # Correct! Scaler is fit to the train data only\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Feature scaling: K-NN works better with normalized data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_knn(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hey ChatGPT, please make this code output understandable for a beginner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](imgs/confusion.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier  # Or GaussianNB for Naive Bayes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "# Apply PCA to reduce dimensions (you can choose the number of components, here it's set to 2 for visualization)\n",
    "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "predict_knn(X_train_pca, X_test_pca, y_train, y_test)\n",
    "\n",
    "print(X_train_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Brief Overview\n",
    "\n",
    "Principal Component Analysis (PCA) is a technique used for **dimensionality reduction** and **feature extraction** by transforming the original dataset into a new coordinate system. In this new system, the axes (called **principal components**) are ordered by the amount of variance they capture from the data. PCA is commonly used to simplify data, reduce noise, and discover patterns by retaining only the most important features.\n",
    "\n",
    "PCA achieves this transformation by projecting the data onto the **eigenvectors** of the covariance matrix of the data. The **eigenvalues** associated with these eigenvectors tell us how much variance each principal component captures.\n",
    "\n",
    "### Steps of PCA\n",
    "\n",
    "1. **Data Centering**:\n",
    "   PCA requires the data to be **centered**, which means subtracting the mean of each feature (column) from the data. This ensures that the new axes represent variance correctly. If your original data matrix is $X$, then the centered data matrix is:\n",
    "\n",
    "   $$ X_{\\text{centered}} = X - \\mu $$\n",
    "\n",
    "   where $\\mu$ is the mean vector of the data.\n",
    "\n",
    "2. **Covariance Matrix**:\n",
    "   The next step is to compute the **covariance matrix** of the centered data. This matrix represents how the different features vary with respect to each other. For a centered data matrix $X_{\\text{centered}}$, the covariance matrix $\\Sigma$ is calculated as:\n",
    "\n",
    "   $$ \\Sigma = \\frac{1}{n-1} X_{\\text{centered}}^T X_{\\text{centered}} $$\n",
    "\n",
    "   where $n$ is the number of observations.\n",
    "\n",
    "3. **Eigenvalue Decomposition**:\n",
    "   PCA involves finding the **eigenvectors** and **eigenvalues** of the covariance matrix. The eigenvectors represent the directions of maximum variance in the data, and the eigenvalues tell us how much variance is captured along these directions. The eigenvalue equation is:\n",
    "\n",
    "   $$ \\Sigma \\mathbf{v}_k = \\lambda_k \\mathbf{v}_k $$\n",
    "\n",
    "   where:\n",
    "   - $\\mathbf{v}_k$ are the **eigenvectors** of $\\Sigma$, which represent the principal component directions.\n",
    "   - $\\lambda_k$ are the **eigenvalues**, which represent the amount of variance captured by each principal component.\n",
    "\n",
    "   The **first eigenvector** $\\mathbf{v}_1$ captures the direction of **maximum variance** in the data, meaning the first principal component captures more variance than any other component. Each successive eigenvector captures the maximum variance that is orthogonal (uncorrelated) to the previous ones. This property allows us to focus on the first few principal components while still retaining most of the important information in the data.\n",
    "\n",
    "4. **Projection onto the New Axes**:\n",
    "   Once the eigenvectors are found, the original data is projected onto these new directions (principal components). The projection of a data point $\\mathbf{x}_i$ onto an eigenvector $\\mathbf{v}_k$ is given by the dot product:\n",
    "\n",
    "   $$ t_{ik} = \\mathbf{x}_i \\cdot \\mathbf{v}_k $$\n",
    "\n",
    "   This projection transforms the data into a new coordinate system where the axes are the eigenvectors, and the resulting values are the **principal components**. For the entire dataset $X_{\\text{centered}}$, the transformation is:\n",
    "\n",
    "   $$ T = X_{\\text{centered}} V $$\n",
    "\n",
    "   where:\n",
    "   - $T$ is the matrix of principal components (transformed data).\n",
    "   - $X_{\\text{centered}}$ is the centered data matrix.\n",
    "   - $V$ is the matrix of eigenvectors (with each column being an eigenvector).\n",
    "\n",
    "5. **Dimensionality Reduction**:\n",
    "   PCA is often used for dimensionality reduction. Instead of keeping all the principal components, we can retain only the top $k$ components that capture the most variance (i.e., the ones corresponding to the largest eigenvalues). Since the first eigenvector captures the most variance and the next few eigenvectors capture successively less, retaining only the first $k$ eigenvectors is typically sufficient to preserve most of the data's variance. The reduced representation is:\n",
    "\n",
    "   $$ T_k = X_{\\text{centered}} V_k $$\n",
    "\n",
    "   where $V_k$ contains only the first $k$ eigenvectors. This reduces the dimensionality of the data while preserving most of its variance. This step is valid because the first few principal components capture the majority of the variance, meaning the most significant patterns in the data are retained.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Eigenvectors**: Define the directions (principal components) in the new coordinate system.\n",
    "- **Eigenvalues**: Measure the amount of variance captured by each principal component. The larger the eigenvalue, the more important the corresponding principal component.\n",
    "- **Principal Components**: The new variables that result from projecting the original data onto the eigenvectors.\n",
    "\n",
    "### Summary\n",
    "\n",
    "PCA transforms the data into a new space where each axis (principal component) is ordered by how much variance it captures. By projecting the data onto the eigenvectors of the covariance matrix, PCA ensures that the first principal component captures the most variance, the second captures the second most, and so on. Dimensionality reduction is achieved by retaining only the top $k$ principal components, significantly reducing the number of features while retaining most of the information. Since the first few eigenvectors capture the majority of the variance, limiting the transformation to those few components is both computationally efficient and effective for many applications.\n",
    "\n",
    "A nice video introduction can be found here: https://youtu.be/HMOI_lkzW08?si=E4bMYdNaSGZpwqkX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do the PCA once more (step-by-step)\n",
    "1. PCA requires scaling/normalization of the data to work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfx = pd.DataFrame(data=X,columns=df.columns[1:])\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. PCA class import and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit(X_train)\n",
    "\n",
    "# Retrieve the eigenvectors (components)\n",
    "eigenvectors = pca.components_\n",
    "\n",
    "# Retrieve the eigenvalues (explained variance)\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "# Print the eigenvectors (principal components)\n",
    "print(\"Eigenvectors (Principal Components):\")\n",
    "print(eigenvectors)\n",
    "\n",
    "# Print the eigenvalues\n",
    "print(\"\\nEigenvalues:\")\n",
    "print(eigenvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the explained variance ratio\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(x=[i+1 for i in range(len(X_train_pca.explained_variance_ratio_))],\n",
    "            y=X_train_pca.explained_variance_ratio_,\n",
    "           s=200, alpha=0.75,c='orange',edgecolor='k')\n",
    "plt.grid(True)\n",
    "plt.title(\"Explained variance ratio of the \\nfitted principal component vector\\n\",fontsize=25)\n",
    "plt.xlabel(\"Principal components\",fontsize=15)\n",
    "plt.xticks([i+1 for i in range(len(X_train_pca.explained_variance_ratio_))],fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.ylabel(\"Explained variance ratio\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot means that the $1^{st}$ principal component explains about 36% of the total variance in the data and the $2^{nd}$ component explians further 19%. Therefore, if we just consider first two components, they together explain 55% of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Showing better class separation using principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the scaled data set using the fitted PCA object\n",
    "\n",
    "X_train_df = pca.transform(X_train)\n",
    "\n",
    "# Put it in a data frame\n",
    "\n",
    "X_train_df = pd.DataFrame(data=X_train_df)\n",
    "X_train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot the first two columns of this transformed data set with the color set to original ground truth class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_train_numeric, _ = pd.factorize(y_train)\n",
    "\n",
    "# Get unique classes and map each class to a specific level for better visualization\n",
    "unique_classes = np.unique(y_train_numeric)\n",
    "class_levels = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "\n",
    "# Create new levels for each class for better separation in 1D plots\n",
    "y_levels_pc1 = [class_levels[cls] for cls in y_train_numeric]  # Y-levels for PC1\n",
    "x_levels_pc2 = [class_levels[cls] for cls in y_train_numeric]  # X-levels for PC2\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "# 2D scatter plot for the first two principal components\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(X_train_df[0], X_train_df[1], c=y_train_numeric, edgecolors='k', alpha=0.75, s=150)\n",
    "plt.grid(True)\n",
    "plt.title(\"Class separation using first two principal components\\n\", fontsize=20)\n",
    "plt.xlabel(\"Principal component-1\", fontsize=15)\n",
    "plt.ylabel(\"Principal component-2\", fontsize=15)\n",
    "\n",
    "# 1D plot for the first principal component with class levels\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(X_train_df[0], y_levels_pc1, c=y_train_numeric, edgecolors='k', alpha=0.75, s=100)\n",
    "plt.grid(True)\n",
    "plt.title(\"First Principal Component with Class Levels\\n\", fontsize=15)\n",
    "plt.xlabel(\"Principal component-1\", fontsize=12)\n",
    "plt.ylabel(\"Class levels\", fontsize=12)\n",
    "plt.yticks(list(class_levels.values()), labels=unique_classes)\n",
    "\n",
    "# 1D plot for the second principal component with class levels\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(x_levels_pc2, X_train_df[1], c=y_train_numeric, edgecolors='k', alpha=0.75, s=100)\n",
    "plt.grid(True)\n",
    "plt.title(\"Second Principal Component with Class Levels\\n\", fontsize=15)\n",
    "plt.ylabel(\"Principal component-2\", fontsize=12)\n",
    "plt.xlabel(\"Class levels\", fontsize=12)\n",
    "plt.xticks(list(class_levels.values()), labels=unique_classes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUIZ TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "\n",
    "Both student A and student B want to perform a PCA before learning a classfier. Who deserves a **neck slap**?\n",
    "\n",
    "![Neck Slap GIF](https://media.giphy.com/media/9U5J7JpaYBr68/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Student A\n",
    "\n",
    "df = pd.read_csv('sample.csv')\n",
    "\n",
    "X = df.drop('Target',axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "# ...\n",
    "\n",
    "# Student B\n",
    "\n",
    "df = pd.read_csv('sample.csv')\n",
    "\n",
    "X = df.drop('Target',axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Both student A and student B deserve a neck slap**. Performing scaling or PCA on the entire dataset before splitting introduces data leakage. Data leakage occurs when information from the test set \"leaks\" into the training set during preprocessing steps. This leads to overly optimistic performance estimates because the model has indirectly gained access to the distribution of the test data, which it would not have in a real-world scenario. **Rather do it like student C:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Student C\n",
    "\n",
    "df = pd.read_csv('sample.csv')\n",
    "\n",
    "X = df.drop('Target',axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=2)  # Adjust the number of components as needed\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: \n",
    "\n",
    "What is the role of eigenvalues and eigenvectors in PCA, and how do they relate to principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PCA, the eigenvectors of the covariance matrix define the directions of maximum variance in the data, and the eigenvalues represent how much variance is captured along each of these directions. The principal components are the projections of the original data onto these eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: \n",
    "\n",
    "Explain why the first few principal components are sufficient for dimensionality reduction. How does the amount of variance captured by the principal components influence this decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few principal components are sufficient for dimensionality reduction because they capture the majority of the variance in the data. The principal components are ordered by the amount of variance they capture, with the first component capturing the most variance and each subsequent component capturing progressively less. By retaining only the top components, we reduce the dimensionality while still preserving most of the information in the data, as the smaller components often represent noise or minor details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: \n",
    "\n",
    "Why is it important to center the data before applying PCA? What would happen if you skipped this step?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centering the data (subtracting the mean of each feature) ensures that PCA measures the directions of variance correctly. If the data is not centered, the principal components might reflect both the variance in the data and the mean of the data, which could distort the results. Without centering, the first principal component might not point in the direction of maximum variance, as it would be influenced by the offset due to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: \n",
    "\n",
    "Describe the process of projecting the original data onto the new axes defined by the eigenvectors. What do we achieve by doing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the eigenvectors (principal components) are computed, the original data is projected onto these new axes by taking the dot product of the data points with the eigenvectors. This transformation results in a new set of coordinates for each data point in the principal component space. By projecting the data, we achieve a new representation where the axes are uncorrelated, and the first few components capture the most important information (variance) in the data. This allows us to reduce the dimensionality of the data while preserving the most critical structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: \n",
    "\n",
    "Given the previous PCA, identify which original feature contributes the most to the first principal component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest absolute value of the first eigenvector is 0.42913765. It is found in the 7-th index and thus corresponds to **Flavanoids**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question/Task 7:\n",
    "\n",
    "Please perform a dimensionality reduction of the full iris.csv dataset (*Name* column is the target) with PCA (dim=2). Create a nice color-coded plot to highlight the classes. How would you rate the PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# Prepare the data\n",
    "X = df.drop('Name', axis=1)\n",
    "y = df['Name']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA to reduce the data to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a DataFrame with the PCA results and the target variable for plotting\n",
    "df_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['Name'] = y\n",
    "\n",
    "# Plot the PCA result with color coding for each species\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Name', data=df_pca, palette='Set1', s=100)\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question/Task 8:\n",
    "\n",
    "Using the Iris dataset, apply PCA for dimensionality reduction and train a classifier (Naive Bayes) using the transformed data. Create a single plot that visualizes both the true class labels and the predicted class labels for the test data, ensuring that both are distinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Solution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "# Prepare the data\n",
    "X = df.drop('Name', axis=1)\n",
    "y = df['Name']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features (scaling only on the training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Perform PCA to reduce the data to 2 components (only on the training set)\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict the test set labels\n",
    "y_pred = model.predict(X_test_pca)\n",
    "\n",
    "# Calculate accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Create a DataFrame with PCA results and predictions for plotting\n",
    "df_test_pca = pd.DataFrame(data=X_test_pca, columns=['PC1', 'PC2'])\n",
    "df_test_pca['Real Class'] = y_test.values\n",
    "df_test_pca['Predicted Class'] = y_pred\n",
    "\n",
    "# Plot the test set with real and predicted class annotations\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot the real classes as larger circles\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Real Class', data=df_test_pca, palette='Set1', \n",
    "                marker='o', s=150, edgecolor='black', legend='brief')\n",
    "\n",
    "# Plot the predicted classes as smaller crosses\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='Predicted Class', data=df_test_pca, palette='Set1', \n",
    "                marker='X', s=70, legend=False)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('PCA of Test Data: Real vs Predicted Class (Circles: Real, Crosses: Predicted)')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
